# -*- coding: utf-8 -*-
"""demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1acygNlFf9wlPOtkcV7xqPOXjHoC0KBvG

A cute little demo showing the simplest usage of minGPT. Configured to run fine on Macbook Air in like a minute.
"""


import torch
from torch.utils.data import Dataset
from torch.utils.data.dataloader import DataLoader

import torchtext
from torchtext.data import get_tokenizer

import json
import torch
from torch.utils.data import Dataset
from torchvision import datasets
from torchvision.transforms import ToTensor
import matplotlib.pyplot as plt

with open('predictive_text_datasets.json') as file_object:
        # store file data in object
        datasets = json.load(file_object)
        dataset = datasets["childrens_book"]
        #dataset = datasets["verb_tenses"]
        #dataset = datasets["simple_verb_tenses"]
        #dataset = datasets["countries_and_languages_medium"]
print(dataset)

num_epochs = 500
hidden_size = 256
learning_rate = 0.001
print_interval = 10



tokenizer = get_tokenizer("basic_english")
# tokenize dataset
dataset = [tokenizer(sentence) for sentence in dataset]



vocabulary = []
print(dataset)
for sentence in dataset:
    for word in sentence:
        if word not in vocabulary:
            vocabulary.append(word)

print(vocabulary)


# encode dataset
for i in range(len(dataset)):
    for j in range(len(dataset[i])):
        dataset[i][j] = vocabulary.index(dataset[i][j])



def get_one_hot_sentence_tensor(sentence:list):
    out_tensor = torch.zeros(len(sentence), 1, len(vocabulary))
    for i in range(len(sentence)):
        out_tensor[i][0][sentence[i]] = 1

    return out_tensor

def word_to_one_hot_tensor(word:str):
    out_tensor = torch.zeros(1, len(vocabulary))
    index = vocabulary.index(word)
    out_tensor[0][index] = 1
    return out_tensor

def one_hot_tensor_to_word(tensor):
    out_word = ""
    for i in range(len(tensor[0])):
        if tensor[0][i] == 1:
            out_word = vocabulary[i]
    return out_word



print(dataset)

from re import X
import pickle

class NextWordDataset(Dataset):
    """
    Dataset for the Sort problem. E.g. for problem length 6:
    Input: 0 0 2 1 0 1 -> Output: 0 0 0 1 1 2
    Which will feed into the transformer concatenated as:
    input:  0 0 2 1 0 1 0 0 0 1 1
    output: I I I I I 0 0 0 1 1 2
    where I is "ignore", as the transformer is reading the input sequence
    """

    def __init__(self, data):
      self.dataset = data

      self.max_sentence_len=0
      for sentence in data:
        if len(sentence) > self.max_sentence_len:
          self.max_sentence_len = len(sentence)

      self.input =[]
      self.output = []

      for sentence in data:
        for i in range(len(sentence)-1):
          x = [-1]*(self.max_sentence_len - i - 1)
          x += sentence[:i+1]
          y = sentence[i+1]

          y = x[1:] + [sentence[i+1]]

          self.input.append(x)
          self.output.append(y)



    def __len__(self):
        return len(self.input)


    def __getitem__(self, idx):
        x = torch.tensor(self.input[idx])
        y = torch.tensor(self.output[idx])
        print(x.shape, y.shape)
        return x, y


# print an example instance of the dataset





train_dataset = NextWordDataset(dataset)

x, y = train_dataset[0]
for a, b in zip(x,y):
    print(int(a),int(b))

# create a GPT instance
from mingpt.model import GPT

model_config = GPT.get_default_config()
model_config.model_type = 'gpt-nano'
model_config.vocab_size = len(vocabulary)
model_config.block_size = train_dataset.max_sentence_len

print("vocab_size" + str(model_config.vocab_size))
print("block size" + str(model_config.block_size))
model = GPT(model_config)

# create a Trainer object
from mingpt.trainer import Trainer

train_config = Trainer.get_default_config()
train_config.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster
train_config.max_iters = 2000
train_config.num_workers = 0
trainer = Trainer(train_config, model, train_dataset)

def batch_end_callback(trainer):
    if trainer.iter_num % 100 == 0:
        print(f"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}")
trainer.set_callback('on_batch_end', batch_end_callback)

trainer.run()

# now let's perform some evaluation
model.eval();

def eval_split(trainer, split, max_batches):
    dataset = {'train':train_dataset, 'test':test_dataset}[split]
    n = train_dataset.length # naugy direct access shrug
    results = []
    mistakes_printed_already = 0
    loader = DataLoader(dataset, batch_size=100, num_workers=0, drop_last=False)
    for b, (x, y) in enumerate(loader):
        x = x.to(trainer.device)
        y = y.to(trainer.device)
        # isolate the input pattern alone
        inp = x[:, :n]
        sol = y[:, -n:]
        # let the model sample the rest of the sequence
        cat = model.generate(inp, n, do_sample=False) # using greedy argmax, not sampling
        sol_candidate = cat[:, n:] # isolate the filled in sequence
        # compare the predicted sequence to the true sequence
        correct = (sol == sol_candidate).all(1).cpu() # Software 1.0 vs. Software 2.0 fight RIGHT on this line haha
        for i in range(x.size(0)):
            results.append(int(correct[i]))
            if not correct[i] and mistakes_printed_already < 3: # only print up to 5 mistakes to get a sense
                mistakes_printed_already += 1
                print("GPT claims that %s sorted is %s but gt is %s" % (inp[i].tolist(), sol_candidate[i].tolist(), sol[i].tolist()))
        if max_batches is not None and b+1 >= max_batches:
            break
    rt = torch.tensor(results, dtype=torch.float)
    print("%s final score: %d/%d = %.2f%% correct" % (split, rt.sum(), len(results), 100*rt.mean()))
    return rt.sum()

# run a lot of examples from both train and test through the model and verify the output correctness
with torch.no_grad():
    train_score = eval_split(trainer, 'train', max_batches=50)
    test_score  = eval_split(trainer, 'test',  max_batches=50)

# let's run a random given sequence through the model as well
n = train_dataset.length # naugy direct access shrug
inp = torch.tensor([[0, 0, 2, 1, 0, 1]], dtype=torch.long).to(trainer.device)
assert inp[0].nelement() == n
with torch.no_grad():
    cat = model.generate(inp, n, do_sample=False)
sol = torch.sort(inp[0])[0]
sol_candidate = cat[:, n:]
print('input sequence  :', inp.tolist())
print('predicted sorted:', sol_candidate.tolist())
print('gt sort         :', sol.tolist())
print('matches         :', bool((sol == sol_candidate).all()))
